{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyber_security_anomaly_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s86xl8NTxrH4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "wcegKb7sHUsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWFiEMmF1czE",
        "outputId": "693b6d11-1844-499a-ab2e-68e39df61b2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=pd.read_csv('/content/drive/MyDrive/AML_assignment_2/UNSW_NB15_training-set.csv')\n",
        "data_test=pd.read_csv('/content/drive/MyDrive/AML_assignment_2/UNSW_NB15_testing-set.csv')"
      ],
      "metadata": {
        "id": "9uMkNzrw9o45"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fFGscr4B1bIJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.concat([data_train,data_test],ignore_index=True)\n",
        "y=data['label']\n",
        "data.drop('label',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "XCstOtfL_psH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring metrics"
      ],
      "metadata": {
        "id": "LsmhnJ5MD8Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_cat={}\n",
        "cat_features=[]\n",
        "for column in data.columns:\n",
        "  if len(data[column].unique())<=12:\n",
        "    unique_cat[column]=data[column].value_counts()/data.shape[0]\n",
        "  if type(data[column][3])==str:\n",
        "    cat_features.append(column)\n"
      ],
      "metadata": {
        "id": "ppLCHSUf92qD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in unique_cat:\n",
        "  print(unique_cat[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gubt2an_k5Z",
        "outputId": "6795f6fe-5785-40c8-8235-4f9483645d46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIN    0.454700\n",
            "INT    0.451883\n",
            "CON    0.078138\n",
            "REQ    0.014875\n",
            "RST    0.000326\n",
            "ECO    0.000047\n",
            "ACC    0.000016\n",
            "CLO    0.000004\n",
            "PAR    0.000004\n",
            "URN    0.000004\n",
            "no     0.000004\n",
            "Name: state, dtype: float64\n",
            "0      0.467410\n",
            "252    0.310172\n",
            "29     0.217807\n",
            "60     0.004238\n",
            "30     0.000167\n",
            "31     0.000147\n",
            "253    0.000035\n",
            "32     0.000016\n",
            "254    0.000008\n",
            "Name: dttl, dtype: float64\n",
            "2    0.450781\n",
            "1    0.297233\n",
            "0    0.224455\n",
            "6    0.014491\n",
            "3    0.012927\n",
            "4    0.000109\n",
            "5    0.000004\n",
            "Name: ct_state_ttl, dtype: float64\n",
            "0    0.987407\n",
            "1    0.012493\n",
            "4    0.000062\n",
            "2    0.000039\n",
            "Name: is_ftp_login, dtype: float64\n",
            "0    0.987399\n",
            "1    0.012477\n",
            "2    0.000062\n",
            "4    0.000062\n",
            "Name: ct_ftp_cmd, dtype: float64\n",
            "0     0.901348\n",
            "1     0.091604\n",
            "4     0.005138\n",
            "9     0.000838\n",
            "2     0.000357\n",
            "16    0.000186\n",
            "6     0.000163\n",
            "12    0.000140\n",
            "30    0.000116\n",
            "25    0.000097\n",
            "3     0.000012\n",
            "Name: ct_flw_http_mthd, dtype: float64\n",
            "0    0.985726\n",
            "1    0.014274\n",
            "Name: is_sm_ips_ports, dtype: float64\n",
            "Normal            0.360923\n",
            "Generic           0.228472\n",
            "Exploits          0.172797\n",
            "Fuzzers           0.094096\n",
            "DoS               0.063464\n",
            "Reconnaissance    0.054282\n",
            "Analysis          0.010389\n",
            "Backdoor          0.009039\n",
            "Shellcode         0.005864\n",
            "Worms             0.000675\n",
            "Name: attack_cat, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that for 'is_sm_ips_ports', 'ct_ftp_cmd', 'is_ftp_login' more than 98% of column equals to specififc value. So we can remove these features. Also we exclude 'proto' because it contains more than 130 unique "
      ],
      "metadata": {
        "id": "6MeX4F5TEBfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constant_columns=['is_sm_ips_ports', 'ct_ftp_cmd', 'is_ftp_login','proto']\n",
        "data.drop(columns=constant_columns,inplace=True)\n"
      ],
      "metadata": {
        "id": "3LWzTNhH-Dqu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features.remove('proto')"
      ],
      "metadata": {
        "id": "adgnZ-A_OSR9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill missing values"
      ],
      "metadata": {
        "id": "OVWBus0cRZQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "for column in data.columns[1:]: \n",
        "  imp_mean = SimpleImputer( strategy='most_frequent')\n",
        "  imp_mean.fit(np.array(data[column]).reshape(-1,1))\n",
        "  data[column]=imp_mean.transform(np.array(data[column]).reshape(-1,1))\n",
        "\n"
      ],
      "metadata": {
        "id": "VxcqgIvnReE5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# for cat in cat_features:\n",
        "#   encoder=LabelEncoder()\n",
        "\n",
        "#   data[cat]=encoder.fit_transform(data[cat])\n",
        "\n",
        "data=pd.get_dummies(data,columns=cat_features)"
      ],
      "metadata": {
        "id": "24lesBas_2zj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "data = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "WCntDZs7P8ax"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( data, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KHb_VOWDcDp8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional GAN"
      ],
      "metadata": {
        "id": "yBxsQeknj1ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "metadata": {
        "id": "ZKBi34u9j3mi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JlUt_j5nqvXZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Undercomplete\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, data_size,n_classes=2):\n",
        "      super(Generator, self).__init__()\n",
        "\n",
        "      self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "      input_size=n_classes + data_size\n",
        "      self.hidden_layer1 = nn.Sequential(\n",
        "          nn.Linear(input_size, 128),\n",
        "          nn.LeakyReLU(0.2)\n",
        "\n",
        "      )\n",
        "\n",
        "      self.hidden_layer2 = nn.Sequential(\n",
        "          nn.Linear(128, 256),\n",
        "          nn.LeakyReLU(0.2)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer3 = nn.Sequential(\n",
        "          nn.Linear(256, 512),\n",
        "          nn.LeakyReLU(0.2),\n",
        "\n",
        "      )\n",
        "      self.hidden_layer4 = nn.Sequential(\n",
        "          nn.Linear(512, 34),\n",
        "          nn.Tahn()\n",
        "\n",
        "      )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, data,labels,n_classes=2):\n",
        "\n",
        "      gen_input = torch.cat((self.label_emb(labels), data), -1)\n",
        "      x = get_input\n",
        "      output=self.hidden_layer1(x)\n",
        "      output=self.hidden_layer2(output)\n",
        "      output=self.hidden_layer3(output)\n",
        "      output=self.hidden_layer4(output)\n",
        "      return output.to(device)\n",
        "        \n",
        "        \n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_size,n_classes):\n",
        "\n",
        "      super(Discriminator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "      # Step 1 : Define the encoder \n",
        "      # Step 2 : Define the decoder\n",
        "      # Step 3 : Initialize the weights (optional)\n",
        "      input_size=n_classes + data_size\n",
        "      \n",
        "      self.hidden_layer1 = nn.Sequential(\n",
        "          nn.Linear(input_size, 512),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "\n",
        "      self.hidden_layer2 = nn.Sequential(\n",
        "          nn.Linear(512, 256),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer3 = nn.Sequential(\n",
        "          nn.Linear(256, 128),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer4 = nn.Sequential(\n",
        "          nn.Linear(128,1),\n",
        "          nn.Sigmoid()\n",
        "\n",
        "      )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, data,labels):\n",
        "      # Step 1: Pass the input through encoder to get latent representation\n",
        "      # Step 2: Take latent representation and pass through decoder\n",
        "      gen_input = torch.cat((self.label_emb(labels), data), -1)\n",
        "      x = get_input\n",
        "      output=self.hidden_layer1(x)\n",
        "      output=self.hidden_layer2(output)\n",
        "      output=self.hidden_layer3(output)\n",
        "      output=self.hidden_layer4(output)\n",
        "      return output.to(device)\n",
        "        \n"
      ],
      "metadata": {
        "id": "I3sF9eJequuB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 50\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 2000\n",
        "input_dim=\n",
        "generator=Generator()\n",
        "discriminator=Discriminator()\n",
        "gen_optimizer=torch.optim.SGD(generator.parameters(),lr=learning_rate)\n",
        "dis_optimizer=torch.optim.SGD(discriminator.parameters(),lr=learning_rate)\n",
        "\n",
        "# print(summary(AE,input_size=(1, 64)))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(AE.parameters(),lr = learning_rate)\n",
        "\n",
        "#Create a random dataset\n",
        "data_loader = DataLoader(TensorDataset(torch.from_numpy(full_train)),batch_size=batchSize,shuffle=True)\n",
        "# del full_train"
      ],
      "metadata": {
        "id": "MFprP08Lq43F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "fa2200eb-a0dd-47fd-b575-5b7043dd5f6a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ed30be8756c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'input_size' and 'latent_dim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses=[]\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  for X in data_loader:\n",
        "    X = X[0].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # forward\n",
        "    output = AE(X.float())\n",
        "    loss = criterion(output*X.float()[1], X.float()[0])\n",
        "    #print(output.dtype)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  losses.append(epoch_loss)\n",
        "  # log\n",
        "  #print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "id": "PHqEIyv5q6x0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}