{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyber_security_anomaly_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s86xl8NTxrH4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "wcegKb7sHUsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWFiEMmF1czE",
        "outputId": "51b357a2-0b41-4b58-e516-fc39c00f000d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=pd.read_csv('/content/drive/MyDrive/AML_assignment_2/UNSW_NB15_training-set.csv')\n",
        "data_test=pd.read_csv('/content/drive/MyDrive/AML_assignment_2/UNSW_NB15_testing-set.csv')"
      ],
      "metadata": {
        "id": "9uMkNzrw9o45"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fFGscr4B1bIJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.concat([data_train,data_test],ignore_index=True)\n",
        "y=data['label']\n",
        "data.drop('label',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "XCstOtfL_psH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring metrics"
      ],
      "metadata": {
        "id": "LsmhnJ5MD8Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_cat={}\n",
        "cat_features=[]\n",
        "for column in data.columns:\n",
        "  if len(data[column].unique())<=12:\n",
        "    unique_cat[column]=data[column].value_counts()/data.shape[0]\n",
        "  if type(data[column][3])==str:\n",
        "    cat_features.append(column)\n"
      ],
      "metadata": {
        "id": "ppLCHSUf92qD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in unique_cat:\n",
        "  print(unique_cat[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gubt2an_k5Z",
        "outputId": "9f47d1fa-e1b6-4a82-ba7c-57dc3a043c38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIN    0.454700\n",
            "INT    0.451883\n",
            "CON    0.078138\n",
            "REQ    0.014875\n",
            "RST    0.000326\n",
            "ECO    0.000047\n",
            "ACC    0.000016\n",
            "CLO    0.000004\n",
            "PAR    0.000004\n",
            "URN    0.000004\n",
            "no     0.000004\n",
            "Name: state, dtype: float64\n",
            "0      0.467410\n",
            "252    0.310172\n",
            "29     0.217807\n",
            "60     0.004238\n",
            "30     0.000167\n",
            "31     0.000147\n",
            "253    0.000035\n",
            "32     0.000016\n",
            "254    0.000008\n",
            "Name: dttl, dtype: float64\n",
            "2    0.450781\n",
            "1    0.297233\n",
            "0    0.224455\n",
            "6    0.014491\n",
            "3    0.012927\n",
            "4    0.000109\n",
            "5    0.000004\n",
            "Name: ct_state_ttl, dtype: float64\n",
            "0    0.987407\n",
            "1    0.012493\n",
            "4    0.000062\n",
            "2    0.000039\n",
            "Name: is_ftp_login, dtype: float64\n",
            "0    0.987399\n",
            "1    0.012477\n",
            "2    0.000062\n",
            "4    0.000062\n",
            "Name: ct_ftp_cmd, dtype: float64\n",
            "0     0.901348\n",
            "1     0.091604\n",
            "4     0.005138\n",
            "9     0.000838\n",
            "2     0.000357\n",
            "16    0.000186\n",
            "6     0.000163\n",
            "12    0.000140\n",
            "30    0.000116\n",
            "25    0.000097\n",
            "3     0.000012\n",
            "Name: ct_flw_http_mthd, dtype: float64\n",
            "0    0.985726\n",
            "1    0.014274\n",
            "Name: is_sm_ips_ports, dtype: float64\n",
            "Normal            0.360923\n",
            "Generic           0.228472\n",
            "Exploits          0.172797\n",
            "Fuzzers           0.094096\n",
            "DoS               0.063464\n",
            "Reconnaissance    0.054282\n",
            "Analysis          0.010389\n",
            "Backdoor          0.009039\n",
            "Shellcode         0.005864\n",
            "Worms             0.000675\n",
            "Name: attack_cat, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that for 'is_sm_ips_ports', 'ct_ftp_cmd', 'is_ftp_login' more than 98% of column equals to specififc value. So we can remove these features. Also we exclude 'proto' because it contains more than 130 unique "
      ],
      "metadata": {
        "id": "6MeX4F5TEBfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constant_columns=['is_sm_ips_ports', 'ct_ftp_cmd', 'is_ftp_login','proto']\n",
        "data.drop(columns=constant_columns,inplace=True)\n"
      ],
      "metadata": {
        "id": "3LWzTNhH-Dqu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features.remove('proto')"
      ],
      "metadata": {
        "id": "adgnZ-A_OSR9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyLBiBrvKZ1G",
        "outputId": "aeb13fef-7da9-4201-bb08-f32f75ee516b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['service', 'state', 'attack_cat']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill missing values"
      ],
      "metadata": {
        "id": "OVWBus0cRZQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "for column in data.columns[1:]: \n",
        "  imp_mean = SimpleImputer( strategy='most_frequent')\n",
        "  imp_mean.fit(np.array(data[column]).reshape(-1,1))\n",
        "  data[column]=imp_mean.transform(np.array(data[column]).reshape(-1,1))\n",
        "\n"
      ],
      "metadata": {
        "id": "VxcqgIvnReE5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# for cat in cat_features:\n",
        "#   encoder=LabelEncoder()\n",
        "\n",
        "#   data[cat]=encoder.fit_transform(data[cat])\n",
        "\n",
        "data=pd.get_dummies(data,columns=cat_features)\n"
      ],
      "metadata": {
        "id": "24lesBas_2zj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZHZZd5exKeu7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "data = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "WCntDZs7P8ax"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( data, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KHb_VOWDcDp8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional GAN"
      ],
      "metadata": {
        "id": "yBxsQeknj1ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "metadata": {
        "id": "ZKBi34u9j3mi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JlUt_j5nqvXZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJuFz2i-UFS-",
        "outputId": "52df6ca1-bab0-4bd1-d524-c1a9e681209a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_size=X_train.shape[1]\n",
        "n_classes=2\n"
      ],
      "metadata": {
        "id": "EjKoDrUtDVr3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class df(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.transform = transform\n",
        "        self.labels = y\n",
        "        self.data=X\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        data=self.data[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "Vq70OjAyJ7wd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Undercomplete\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, data_size=data_size,n_classes=n_classes):\n",
        "      super(Generator, self).__init__()\n",
        "\n",
        "      self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "      input_size=n_classes + data_size\n",
        "      self.hidden_layer1 = nn.Sequential(\n",
        "          nn.Linear(input_size, 128),\n",
        "          nn.LeakyReLU(0.2)\n",
        "\n",
        "      )\n",
        "\n",
        "      self.hidden_layer2 = nn.Sequential(\n",
        "          nn.Linear(128, 256),\n",
        "          nn.LeakyReLU(0.2)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer3 = nn.Sequential(\n",
        "          nn.Linear(256, 512),\n",
        "          nn.LeakyReLU(0.2),\n",
        "\n",
        "      )\n",
        "      self.hidden_layer4 = nn.Sequential(\n",
        "          nn.Linear(512, data_size),\n",
        "          nn.Tanh()\n",
        "\n",
        "      )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, data,labels,n_classes=2):\n",
        "\n",
        "      get_input = torch.cat((self.label_emb(labels.to(device)), data.to(device)), -1)\n",
        "      x = get_input\n",
        "      output=self.hidden_layer1(x)\n",
        "      output=self.hidden_layer2(output)\n",
        "      output=self.hidden_layer3(output)\n",
        "      output=self.hidden_layer4(output)\n",
        "      return output.to(device)\n",
        "        \n",
        "        \n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_size=data_size,n_classes=n_classes):\n",
        "\n",
        "      super(Discriminator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "      # Step 1 : Define the encoder \n",
        "      # Step 2 : Define the decoder\n",
        "      # Step 3 : Initialize the weights (optional)\n",
        "      input_size = n_classes + data_size\n",
        "\n",
        "      self.hidden_layer1 = nn.Sequential(\n",
        "          nn.Linear(input_size, 512),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "\n",
        "      self.hidden_layer2 = nn.Sequential(\n",
        "          nn.Linear(512, 256),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer3 = nn.Sequential(\n",
        "          nn.Linear(256, 128),\n",
        "          nn.LeakyReLU(0.2),\n",
        "          nn.Dropout(0.3)\n",
        "\n",
        "      )\n",
        "      self.hidden_layer4 = nn.Sequential(\n",
        "          nn.Linear(128,1)\n",
        "\n",
        "      )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, data,labels):\n",
        "      # Step 1: Pass the input through encoder to get latent representation\n",
        "      # Step 2: Take latent representation and pass through decoder\n",
        "      get_input = torch.cat((self.label_emb(labels), data), -1)\n",
        "      x = get_input\n",
        "      output=self.hidden_layer1(x)\n",
        "      output=self.hidden_layer2(output)\n",
        "      output=self.hidden_layer3(output)\n",
        "      output=self.hidden_layer4(output)\n",
        "      return output.to(device)\n",
        "        \n"
      ],
      "metadata": {
        "id": "I3sF9eJequuB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize=50\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 20\n",
        "generator=Generator()\n",
        "discriminator=Discriminator()\n",
        "if torch.cuda.is_available():\n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "\n",
        "# print(summary(AE,input_size=(1, 64)))\n",
        "\n",
        "criterion_gen = nn.KLDivLoss()\n",
        "criterion=nn.MSELoss()\n",
        "if torch.cuda.is_available():\n",
        "  criterion.cuda()\n",
        "  criterion_gen.cuda()\n",
        "X_tensor_train = torch.tensor(X_train).float().to(device)\n",
        "X_tensor_test = torch.tensor(X_test).float().to(device)\n",
        "y_tensor_train = torch.tensor(y_train.values).float().to(device)\n",
        "y_tensor_test = torch.tensor(y_test.values).float().to(device)\n",
        "\n",
        "\n",
        "\n",
        "data_loader = DataLoader(df(X_tensor_train,y_tensor_train),batch_size=batchSize,shuffle=True)\n",
        "\n",
        "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "dis_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "#Create a random dataset\n",
        "# del full_train"
      ],
      "metadata": {
        "id": "MFprP08Lq43F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2yBbc9aPXnj",
        "outputId": "8e90aa96-a896-403e-ab86-b9d1084e2ff5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor"
      ],
      "metadata": {
        "id": "JIrmsl3aT6bA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "dZVEB2R6nlgH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses=[]\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  gen_losses=[]\n",
        "  dis_losses=[]\n",
        "\n",
        "  for i, (x,y) in enumerate(data_loader):\n",
        "\n",
        "      batch_size = x.shape[0]\n",
        "\n",
        "      # Adversarial ground truths\n",
        "      valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "      # Configure input\n",
        "      real_data = Variable(x.type(FloatTensor))\n",
        "      labels = Variable(y.type(LongTensor))\n",
        "\n",
        "      # -----------------\n",
        "      #  Train Generator\n",
        "      # -----------------\n",
        "\n",
        "      gen_optimizer.zero_grad()\n",
        "\n",
        "      # Sample noise and labels as generator input\n",
        "      z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, data.shape[1]))))\n",
        "      gen_labels = Variable(LongTensor(np.random.randint(0, n_classes,batch_size)))\n",
        "\n",
        "      # Generate new data\n",
        "      gen_imgs = generator(z.to(device), gen_labels.to(device))\n",
        "\n",
        "      # Loss measures generator's ability to fool the discriminator\n",
        "      validity = discriminator(gen_imgs, gen_labels)\n",
        "      g_loss = criterion_gen(validity, valid)\n",
        "      gen_losses.append(g_loss.item())\n",
        "      g_loss.backward()\n",
        "      gen_optimizer.step()\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "\n",
        "      dis_optimizer.zero_grad()\n",
        "\n",
        "      # Loss for real data\n",
        "      validity_real = discriminator(real_data, labels)\n",
        "      d_real_loss = criterion(validity_real, valid)\n",
        "\n",
        "      # Loss for fake data\n",
        "      validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
        "      d_fake_loss = criterion(validity_fake, fake)\n",
        "\n",
        "      # Total discriminator loss\n",
        "      d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "      dis_losses.append(d_loss.item())\n",
        "      d_loss.backward()\n",
        "      dis_optimizer.step()\n",
        "      # if i%0==0:\n",
        "      #   print(\n",
        "      #       \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "      #       % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
        "      #   )\n",
        "\n",
        "      # batches_done = epoch * len(data_loader) + i\n",
        "  losses.append((np.mean(gen_losses),np.mean(dis_losses)))\n",
        "\n",
        "        # if batches_done % opt.sample_interval == 0:\n",
        "        #     sample_image(n_row=10, batches_done=batches_done)\n",
        "  # log\n",
        "  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, d_loss.item()))"
      ],
      "metadata": {
        "id": "PHqEIyv5q6x0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6c6e00-e4f4-4d3c-994c-681dafa7fbaa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2887: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            "  5%|▌         | 1/20 [00:28<09:00, 28.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/20], loss:1.4958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [01:09<10:50, 36.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [2/20], loss:0.5132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [01:41<09:37, 33.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [3/20], loss:38915.4023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [02:09<08:24, 31.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [4/20], loss:0.1258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [02:37<07:37, 30.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [5/20], loss:5453.4214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [03:04<06:50, 29.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [6/20], loss:0.1589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [03:38<06:40, 30.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [7/20], loss:0.0723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [04:07<06:03, 30.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [8/20], loss:0.6828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [04:36<05:28, 29.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [9/20], loss:1.6246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [05:06<04:56, 29.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [10/20], loss:0.1755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [05:38<04:35, 30.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [11/20], loss:0.0974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [06:06<03:57, 29.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [12/20], loss:0.6522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [06:36<03:27, 29.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [13/20], loss:0.2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [07:07<03:00, 30.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [14/20], loss:0.1388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [07:36<02:28, 29.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [15/20], loss:0.2880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [08:05<01:57, 29.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [16/20], loss:0.2404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [08:32<01:26, 28.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [17/20], loss:2.7767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [09:07<01:01, 30.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [18/20], loss:6202.6152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [09:34<00:29, 29.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [19/20], loss:24361.7559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [10:06<00:00, 30.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [20/20], loss:0.9651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(generator, '/content/drive/MyDrive/AML_assignment_2/gen.pth')\n",
        "torch.save(discriminator, '/content/drive/MyDrive/AML_assignment_2/dis.pth')"
      ],
      "metadata": {
        "id": "fOytnxAbrXLW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen=torch.load( '/content/drive/MyDrive/AML_assignment_2/gen.pth')\n",
        "dis=torch.load( '/content/drive/MyDrive/AML_assignment_2/dis.pth')"
      ],
      "metadata": {
        "id": "rX4SCxLVxQ6b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator loss"
      ],
      "metadata": {
        "id": "xq-_umz-fnp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temp=[loss[1] for loss in losses ]\n",
        "plt.plot(range(len(temp)),temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "_pZ6CARnxZRZ",
        "outputId": "2b5ec734-c9fc-4d52-d9a2-fd26d91d6d5a"
      },
      "execution_count": 24,
      "outputs": [
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator loss"
      ],
      "metadata": {
        "id": "v0NlsX8zfxnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "temp=[loss[0] for loss in losses ]\n",
        "plt.plot(range(len(temp)),temp)"
      ],
      "metadata": {
        "id": "SCU1A_Vvf0LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save losses"
      ],
      "metadata": {
        "id": "Fk1CoUGUfuFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l={}\n",
        "l['dis loss']=[loss[1] for loss in losses ]\n",
        "l['gen loss']=[loss[0] for loss in losses ]\n"
      ],
      "metadata": {
        "id": "TovTyDDTgUu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_fd=pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "eDoq-tyhv_rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_fd.to_csv('/content/drive/MyDrive/AML_assignment_2/loss.csv')"
      ],
      "metadata": {
        "id": "OcOWBPp8wFFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After generating the network intrusion and balanced the data, it is important to see if balancing the data helped to improve a ML classifier perfomence. There are a lot of ML classifiers that can be used. For this assignment we will only take 3 into consideration. The 3 classifiers are Random Forest, Explainable Boosting Machine and Classical Neural Network.\n",
        "\n"
      ],
      "metadata": {
        "id": "-BenjoDygVgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fdKepPsuxQVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balance data. Generate new observations with labels equal to 0."
      ],
      "metadata": {
        "id": "2X9LmvJro4_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disbalance=y_train.value_counts()[1]-y_train.value_counts()[0]"
      ],
      "metadata": {
        "id": "Jimiap3IkgKo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disbalance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFkxbFy2m2S7",
        "outputId": "d02a7918-4816-4da1-e3c1-261ba94f84f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57488"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#Generate to noise to input of generator\n",
        "\n",
        "z = Variable(FloatTensor(np.random.normal(0, 1, (57000, data.shape[1]))))\n",
        "gen_labels = Variable(LongTensor(np.zeros(57000)))\n",
        "\n",
        "with torch.no_grad():\n",
        "  new_data = gen(z, gen_labels).cpu().numpy()"
      ],
      "metadata": {
        "id": "PZRoVxQNpozh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ext = np.vstack((X_train, new_data))\n",
        "y_train_ext = np.hstack((y_train, np.zeros(57000)))"
      ],
      "metadata": {
        "id": "AO36FjXspqBl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "px1TknQ8y6mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d6677d-9044-48ab-89b5-b265c435c0d8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    131813\n",
              "0     74325\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train_ext, return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V8-X98UhaQU",
        "outputId": "6d9b4c91-433b-4df3-9b54-3f43576531f3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([131325, 131813]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare metrics of classification for extended dataset and dataset without generated data."
      ],
      "metadata": {
        "id": "mcI3VdMXh4Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After generating the network intrusion and balanced the data, We will check if balancing the data helped to improve a ML classifier perfomence. For this assignment we will only take 3 into consideration. The 3 classifiers are Random Forest, Explainable Boosting Machine and Classical Neural Network."
      ],
      "metadata": {
        "id": "h6IFifssiVM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install interpret"
      ],
      "metadata": {
        "id": "83sJnVXXjW7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from interpret.glassbox import ExplainableBoostingClassifier"
      ],
      "metadata": {
        "id": "_w1XV26Xhqnq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "jddxwYIikCC9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "def cross_validation(model, _X, _y, _cv=5):\n",
        "      '''Function to perform 5 Folds Cross-Validation\n",
        "       Parameters\n",
        "       ----------\n",
        "      model: Python Class, default=None\n",
        "              This is the machine learning algorithm to be used for training.\n",
        "      _X: array\n",
        "           This is the matrix of features.\n",
        "      _y: array\n",
        "           This is the target variable.\n",
        "      _cv: int, default=5\n",
        "          Determines the number of folds for cross-validation.\n",
        "       Returns\n",
        "       -------\n",
        "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
        "       'recall', 'f1' for both training set and validation set.\n",
        "      '''\n",
        "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "      results = cross_validate(estimator=model,\n",
        "                               X=_X,\n",
        "                               y=_y,\n",
        "                               cv=_cv,\n",
        "                               scoring=_scoring,\n",
        "                               return_train_score=True)\n",
        "      \n",
        "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
        "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
        "              \"Training Precision scores\": results['train_precision'],\n",
        "              \"Mean Training Precision\": results['train_precision'].mean(),\n",
        "              \"Training Recall scores\": results['train_recall'],\n",
        "              \"Mean Training Recall\": results['train_recall'].mean(),\n",
        "              \"Training F1 scores\": results['train_f1'],\n",
        "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
        "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
        "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
        "              \"Validation Precision scores\": results['test_precision'],\n",
        "              \"Mean Validation Precision\": results['test_precision'].mean(),\n",
        "              \"Validation Recall scores\": results['test_recall'],\n",
        "              \"Mean Validation Recall\": results['test_recall'].mean(),\n",
        "              \"Validation F1 scores\": results['test_f1'],\n",
        "              \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
        "              }"
      ],
      "metadata": {
        "id": "ZaTEwpLbk8NN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest classifier\n",
        "\n",
        " without extended data"
      ],
      "metadata": {
        "id": "6-8pSS6hj5rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MHPujXFkk6SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Random forest')\n",
        "rfc=RandomForestClassifier()\n",
        "# rfc.fit(X_train,y_train)\n",
        "# y_pred = rfc.predict(X_test)\n",
        "# print(classification_report(y_test, y_pred))\n",
        "rfc_res=cross_validation(rfc, X_train, y_train, _cv=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTixJP39je7o",
        "outputId": "4ae2af29-8dcd-4c25-df14-78e65b109374"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeiYnGjOlkz8",
        "outputId": "ca5095b0-9fe2-4d36-9cf7-699977a9acdf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Training Accuracy scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Training Accuracy': 100.0,\n",
              " 'Training Precision scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Training Precision': 1.0,\n",
              " 'Training Recall scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Training Recall': 1.0,\n",
              " 'Training F1 scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Training F1 Score': 1.0,\n",
              " 'Validation Accuracy scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Validation Accuracy': 100.0,\n",
              " 'Validation Precision scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Validation Precision': 1.0,\n",
              " 'Validation Recall scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Validation Recall': 1.0,\n",
              " 'Validation F1 scores': array([1., 1., 1., 1., 1.]),\n",
              " 'Mean Validation F1 Score': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add extended data"
      ],
      "metadata": {
        "id": "roSaLL8fkGvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Random forest')\n",
        "rfc=RandomForestClassifier()\n",
        "rfc.fit(X_train_ext,y_train_ext)\n",
        "y_pred = rfc.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaW8C3vVj3kc",
        "outputId": "07ed5e7f-e773-4ef2-90e1-883599fc11d2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     18675\n",
            "           1       1.00      1.00      1.00     32860\n",
            "\n",
            "    accuracy                           1.00     51535\n",
            "   macro avg       1.00      1.00      1.00     51535\n",
            "weighted avg       1.00      1.00      1.00     51535\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bG8OaUcTlwfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier"
      ],
      "metadata": {
        "id": "6R4CNnVWlMV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without synthetic data"
      ],
      "metadata": {
        "id": "liqvP7MZmCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpl = MLPClassifier(hidden_layer_sizes=(50))\n",
        "print(cross_validation(mpl, X_train, y_train, _cv=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_jOjeWUkskV",
        "outputId": "3da88450-2f24-4917-86f3-3cd8284312ab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Training Accuracy scores': array([0.99725305, 0.99839306, 0.99710145, 0.99773211, 0.99853861]), 'Mean Training Accuracy': 99.78036549988288, 'Training Precision scores': array([0.99636931, 0.99823792, 0.99965729, 0.99663481, 0.99955367]), 'Mean Training Precision': 0.9980905997230469, 'Training Recall scores': array([0.99934566, 0.99925083, 0.99580844, 0.9998293 , 0.99816028]), 'Mean Training Recall': 0.9984789037655997, 'Training F1 scores': array([0.99785527, 0.99874412, 0.99772915, 0.9982295 , 0.99885649]), 'Mean Training F1 Score': 0.9982829058006756, 'Validation Accuracy scores': array([0.99755021, 0.99805957, 0.99755021, 0.99793824, 0.99837485]), 'Mean Validation Accuracy': 99.78946168288336, 'Validation Precision scores': array([0.99685939, 0.99784066, 0.99977164, 0.99701154, 0.99939226]), 'Mean Validation Precision': 0.99817509785162, 'Validation Recall scores': array([0.99931722, 0.99912757, 0.99639646, 0.9997724 , 0.9980654 ]), 'Mean Validation Recall': 0.9985358103235354, 'Validation F1 scores': array([0.9980868 , 0.9984837 , 0.9980812 , 0.99839006, 0.99872839]), 'Mean Validation F1 Score': 0.9983540279755122}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With extended data"
      ],
      "metadata": {
        "id": "ZRcoCF7vmGPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpl = MLPClassifier(hidden_layer_sizes=(50))\n",
        "print(cross_validation(mpl, X_train_ext, y_train_ext, _cv=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qs2Se7XmBvU",
        "outputId": "5383adca-9297-48aa-c8b7-e5920c52bb8d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Training Accuracy scores': array([0.99859864, 0.99858914, 0.99845613, 0.99899768, 0.99922094]), 'Mean Training Accuracy': 99.87725065615692, 'Training Precision scores': array([0.99723848, 0.99731368, 0.99706842, 0.99980054, 0.99863604]), 'Mean Training Precision': 0.9980114326621112, 'Training Recall scores': array([0.99997155, 0.99987672, 0.99985775, 0.99819822, 0.99981034]), 'Mean Training Recall': 0.9995429151094287, 'Training F1 scores': array([0.99860315, 0.99859356, 0.99846114, 0.99899873, 0.99922284]), 'Mean Training F1 Score': 0.9987758838415907, 'Validation Accuracy scores': array([0.9970548 , 0.99692179, 0.99726381, 0.99868889, 0.99982899]), 'Mean Validation Accuracy': 99.79516550804554, 'Validation Precision scores': array([0.99419219, 0.99422794, 0.99479147, 0.99980991, 0.99992412]), 'Mean Validation Precision': 0.9965891244346503, 'Validation Recall scores': array([0.99996207, 0.99965861, 0.99977241, 0.99757226, 0.99973447]), 'Mean Validation Recall': 0.9993399636363796, 'Validation F1 scores': array([0.99706878, 0.99693588, 0.99727572, 0.99868983, 0.99982928]), 'Mean Validation F1 Score': 0.9979598990996404}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ExplainableBoostingClassifier"
      ],
      "metadata": {
        "id": "rH7a7og4mRPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ebc = ExplainableBoostingClassifier()\n",
        "cross_validation(mpl, X_train, y_train, _cv=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1bcoqF-mQRw",
        "outputId": "872f8e32-3961-48cb-fd7a-26dc31d90af6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Training Accuracy scores': array([0.99763507, 0.99885998, 0.99617367, 0.9980535 , 0.99670125]),\n",
              " 'Mean Training Accuracy': 99.74846949793867,\n",
              " 'Training Precision scores': array([0.99644653, 0.99981956, 0.99986647, 0.99734133, 0.999838  ]),\n",
              " 'Mean Training Precision': 0.9986623797364131,\n",
              " 'Training Recall scores': array([0.99986724, 0.99839734, 0.99414889, 0.99962068, 0.99500242]),\n",
              " 'Mean Training Recall': 0.9974073122368988,\n",
              " 'Training F1 scores': array([0.99815395, 0.99910795, 0.99699948, 0.9984797 , 0.99741435]),\n",
              " 'Mean Training F1 Score': 0.99803108683872,\n",
              " 'Validation Accuracy scores': array([0.99772   , 0.99878723, 0.99626467, 0.99808378, 0.99694375]),\n",
              " 'Mean Validation Accuracy': 99.75598866184656,\n",
              " 'Validation Precision scores': array([0.99674748, 0.99984803, 0.99969495, 0.99742609, 0.99977141]),\n",
              " 'Mean Validation Precision': 0.9986975918363565,\n",
              " 'Validation Recall scores': array([0.99969654, 0.99825513, 0.99446194, 0.99958273, 0.99544799]),\n",
              " 'Mean Validation Recall': 0.9974888672057508,\n",
              " 'Validation F1 scores': array([0.99821983, 0.99905095, 0.99707158, 0.99850325, 0.99760502]),\n",
              " 'Mean Validation F1 Score': 0.9980901239372949}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ebc = ExplainableBoostingClassifier()\n",
        "print(cross_validation(mpl, X_train_ext, y_train_ext, _cv=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9cw6NHVmXXY",
        "outputId": "23e4e70e-758c-45a5-c572-dddb356bae6f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Training Accuracy scores': array([0.99856064, 0.99636122, 0.99857964, 0.99790035, 0.99796685]), 'Mean Training Accuracy': 99.7873739140944, 'Training Precision scores': array([0.99721006, 0.99941797, 0.99718195, 0.99582598, 0.99598576]), 'Mean Training Precision': 0.9971243432244277, 'Training Recall scores': array([0.99992413, 0.99331437, 0.99999052, 1.        , 0.99997155]), 'Mean Training Recall': 0.9986401138519663, 'Training F1 scores': array([0.99856525, 0.99635682, 0.99858426, 0.99790862, 0.99797467]), 'Mean Training F1 Score': 0.9978779263635541, 'Validation Accuracy scores': array([0.99692179, 0.99631375, 0.99739682, 0.99988599, 0.999981  ]), 'Mean Validation Accuracy': 99.80998702857354, 'Validation Precision scores': array([0.9940043 , 0.99881819, 0.99490489, 0.99984828, 1.        ]), 'Mean Validation Precision': 0.9975151320558165, 'Validation Recall scores': array([0.9998862 , 0.99381709, 0.99992414, 0.99992413, 0.99996207]), 'Mean Validation Recall': 0.9987027264438785, 'Validation F1 scores': array([0.99693658, 0.99631137, 0.9974082 , 0.9998862 , 0.99998103]), 'Mean Validation F1 Score': 0.9981046756709866}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ebс_global = ebс.explain_global(name='EBM')\n",
        "show(ebс_global)"
      ],
      "metadata": {
        "id": "imNlQ2bEogHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
